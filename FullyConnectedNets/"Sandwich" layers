"Sandwich" layers

output :
Testing affine_relu_forward:
dx error:  1.0
dw error:  1.0
db error:  1.0

reason:
in the file layers.py  the function relu_forward is not written correctly
originally, assignment is writtend by 
out = x
it means out and x represent the same space, the change to out is also applied to x 

"All arrays generated by basic slicing are always views of the original array"

solution:
out = np.copy(x)

but why the potential problem is not discovered in the step ReLU layer backward ?
in fact, when Testing relu_backward function, dx error is also 1, or 0.333333, not the hint, "The error should be around 3e-12"

i tried to print dx and dout obtained by "eval_numerical_gradient_array" and "relu_backward", it seemed that the difference only
existed when x's elements is equal to zero, and the difference is just 0.5 times.

in order to calculate the same answer, the relu_backward is written by :
    dx = np.ones(x.shape)
    dx[x < 0] = 0
    dx[x == 0] = 0.5
    dx = np.multiply(dx, dout)
    
thus, in the step, i seem to calculate the right answer, but the probles occurs in "Sandwich" layers


usefull introduction about index:
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
